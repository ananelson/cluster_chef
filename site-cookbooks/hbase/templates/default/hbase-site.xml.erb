<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
/**
 * Copyright 2009 The Apache Software Foundation
 *
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
-->
<configuration>
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://<%= @namenode_fqdn %>:8020/hadoop/hbase</value>
    <description>The directory shared by region servers.
    Should be fully-qualified to include the filesystem to use.
    E.g: hdfs://NAMENODE_SERVER:PORT/HBASE_ROOTDIR
    </description>
  </property>
  <property>
    <name>hbase.cluster.distributed</name>
    <value>true</value>
    <description>The mode the cluster will be in. Possible values are
      false: standalone and pseudo-distributed setups with managed Zookeeper
      true: fully-distributed with unmanaged Zookeeper Quorum (see hbase-env.sh)
    </description>
  </property>
  <property>
    <name>hbase.zookeeper.quorum</name>
    <value><%= @zookeeper_address %></value>
    <description>Comma separated list of servers in the ZooKeeper Quorum.
    For example, "host1.mydomain.com,host2.mydomain.com,host3.mydomain.com".
    By default this is set to localhost for local and pseudo-distributed modes
    of operation. For a fully-distributed setup, this should be set to a full
    list of ZooKeeper quorum servers. If HBASE_MANAGES_ZK is set in hbase-env.sh
    this is the list of servers which we will start/stop ZooKeeper on.
    </description>
  </property>

  <!-- Count of RPC Server instances spun up on RegionServers Same property is used by the HMaster for count of master handlers. Default is 25.  -->
  <property>
    <name>hbase.regionserver.handler.count</name>
    <value>100</value>
  </property>

  <!-- Size of the write buffer in bytes. A bigger buffer takes more memory, on both the client and server side since server instantiates the passed write buffer to process it, but reduces the number of RPC. For an estimate of server-side memory-used, evaluate hbase.client.write.buffer * hbase.regionserver.handler.count; default 2097152 -->
  <property>
    <name>hbase.client.write.buffer</name>
    <value>8000000</value>
  </property>

  
  <!-- 	Maximum HStoreFile size. If any one of a column families' HStoreFiles has grown to exceed this value, the hosting HRegion is split in two. Default: 256M. For clusters with lots of data, can be tuned up to 1 GB to result in less regions on the cluster. -->
  <property>
    <name>hbase.hregion.max.filesize</name>
    <value>1073741824</value> <!-- 1GB ; default 256M -->
  </property>

  <!-- Memstore will be flushed to disk if size of the memstore exceeds this number of bytes. default 67108864 = 64mb -->
  <property>
    <name>hbase.hregion.memstore.flush.size</name>
    <value>67108864</value> <!-- 64MB ; default 64MB -->
  </property>

  <!-- Block updates if memstore has hbase.hregion.memstore.block.multiplier times hbase.hregion.flush.size bytes. Useful preventing runaway memstore during spikes in update traffic. Without an upper-bound, memstore fills such that when it flushes the resultant flush files take a long time to compact or split, or worse, we OOME. Controls when to start blocking writes to keep the MemStore size sane. It defaults to 2 (multiplied by the memstore.flush.size). For production clusters with lots of RAM that you monitor closely, you can up to something like 8. -->
  <property>
    <name>hbase.hregion.memstore.block.multiplier</name>
    <value>8</value> <!-- default 2 -->
  </property>

  <!--  Maximum size of all memstores in a region server before new updates are blocked and flushes are forced. Defaults to 40% of heap.  -->
  <property>
    <name>hbase.regionserver.global.memstore.upperLimit</name>
    <value>0.4</value>
  </property>

  <!-- 	Percentage of maximum heap (-Xmx setting) to allocate to block cache used by HFile/StoreFile. Default of 0.2 means allocate 20%. Set to 0 to disable. -->
  <property>
    <name>hfile.block.cache.size</name>
    <value>0.1</value>
  </property>

  <property>
    <description>ZooKeeper session timeout. HBase passes this to the zk quorum as suggested maximum time for a session. See     http://hadoop.apache.org/zookeeper/docs/current/zookeeperProgrammers.html#ch_zkSessions     "The client sends a requested timeout, the server responds with the timeout that it can give the client. " In milliseconds.</description>
    <name>zookeeper.session.timeout</name>
    <value>90000</value> <!-- default 90000 -->
  </property>
  <property>
    <description>hbase.zookeeper.property.tickTime -- The zookeeper.session.timeout must be a minimum of 2 times the tickTime and a maximum of 20 times the tickTime. " For large clusters or latent environments (eg EC2), set it to 6 or 9 even from its current setting of '3'.</description>
    <name>hbase.zookeeper.property.tickTime</name>
    <value>6</value> <!-- default: 3 -->
  </property>  
  
  
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>hbase.tmp.dir</name>
    <value><%= node[:hbase][:tmp_dir] %>/hbase-\${user.name}</value>
  </property>

<% if node[:hbase][:client_scanner_caching] %>
<property>
 <name>hbase.client.scanner.caching</name>
 <value><%= node[:hbase][:client_scanner_caching] %></value>
</property>
<% end %>
 
</configuration>
